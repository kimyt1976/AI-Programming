{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMR/UN3xPlzRfgRs4gPn+qO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimyt1976/AI-Programming/blob/main/%ED%95%B4%EC%96%91%EB%8C%80_10%EC%A3%BC%EC%B0%A8%EA%B3%BC%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9s9aUQzXSbr",
        "outputId": "16a4584f-bd84-4a38-d9e5-9ed01c82936d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train :  [0 1 2] y_train :  [1 4 7]\n",
            "x_test :  [3 4 5] y_test :  [10 13 16]\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_18 (Dense)            (None, 4)                 8         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13 (52.00 Byte)\n",
            "Trainable params: 13 (52.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "2/2 [==============================] - 1s 11ms/step - loss: 19.2150 - accuracy: 0.0000e+00\n",
            "Epoch 2/30\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 19.1542 - accuracy: 0.0000e+00\n",
            "Epoch 3/30\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 19.0590 - accuracy: 0.0000e+00\n",
            "Epoch 4/30\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 18.9858 - accuracy: 0.0000e+00\n",
            "Epoch 5/30\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 18.9026 - accuracy: 0.0000e+00\n",
            "Epoch 6/30\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 18.7981 - accuracy: 0.0000e+00\n",
            "Epoch 7/30\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 18.7444 - accuracy: 0.0000e+00\n",
            "Epoch 8/30\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 18.6525 - accuracy: 0.0000e+00\n",
            "Epoch 9/30\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 18.5550 - accuracy: 0.0000e+00\n",
            "Epoch 10/30\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 18.4890 - accuracy: 0.0000e+00\n",
            "Epoch 11/30\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 18.4028 - accuracy: 0.0000e+00\n",
            "Epoch 12/30\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 18.3162 - accuracy: 0.0000e+00\n",
            "Epoch 13/30\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 18.2395 - accuracy: 0.0000e+00\n",
            "Epoch 14/30\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 18.1271 - accuracy: 0.0000e+00\n",
            "Epoch 15/30\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 18.0597 - accuracy: 0.0000e+00\n",
            "Epoch 16/30\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 17.9545 - accuracy: 0.0000e+00\n",
            "Epoch 17/30\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 17.8846 - accuracy: 0.0000e+00\n",
            "Epoch 18/30\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 17.7942 - accuracy: 0.0000e+00\n",
            "Epoch 19/30\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 17.7046 - accuracy: 0.0000e+00\n",
            "Epoch 20/30\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 17.6155 - accuracy: 0.0000e+00\n",
            "Epoch 21/30\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 17.5372 - accuracy: 0.0000e+00\n",
            "Epoch 22/30\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 17.4227 - accuracy: 0.0000e+00\n",
            "Epoch 23/30\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 17.3542 - accuracy: 0.0000e+00\n",
            "Epoch 24/30\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 17.2651 - accuracy: 0.0000e+00\n",
            "Epoch 25/30\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 17.1764 - accuracy: 0.0000e+00\n",
            "Epoch 26/30\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 17.0881 - accuracy: 0.0000e+00\n",
            "Epoch 27/30\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 16.9999 - accuracy: 0.0000e+00\n",
            "Epoch 28/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 16.9118 - accuracy: 0.0000e+00\n",
            "Epoch 29/30\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 16.8236 - accuracy: 0.0000e+00\n",
            "Epoch 30/30\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 16.7180 - accuracy: 0.0000e+00\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "x_test :  [3 4 5] 예측 :  [1.2895931 1.6733674 2.0571418]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "x_train = np.array([[0], [1], [2]])\n",
        "y_train = 3 * x_train + 1\n",
        "\n",
        "x_test = np.array([[3], [4], [5]])\n",
        "y_test = 3 * x_test + 1\n",
        "\n",
        "print('x_train : ', x_train.flatten(), 'y_train : ', y_train.flatten())\n",
        "print('x_test : ', x_test.flatten(), 'y_test : ', y_test.flatten())\n",
        "\n",
        "#=========================================================================\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(4, input_shape=(1, )))\n",
        "model.add(keras.layers.Dense(1))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#=========================================================================================================================\n",
        "# loss = mean_squared_error, binary_crossentropy, categorical_crossentropy, mean_absolute_error, huber_loss\n",
        "# optimizer = sgd, adam , adagrad , rmsprop\n",
        "#=========================================================================================================================\n",
        "# * 손실함수 - 예측값과 실제값의 차이를 구하는 함수 / 두값의 차이가 클수록 손실함수의 값은 커짐 /\n",
        "#              두값의 차이가 작을수록 손실함수의 값도 작아짐\n",
        "#              손실 함수의 값을 최소화 하는 W, b를 찾아가는것이 목표 /\n",
        "#              회귀 : 평균제곱오차 / 분류 : 크로스 엔트로피\n",
        "# 1) 평균 제곱 오차 (Mean Squared Error, MSE) - 연속형 변수를 예측할 때 사용한다\n",
        "# 2) 크로스 엔트로피 (Cross-Entropy) - 낮은 확률로 예측해서 맞추거나, 높은  확률로 예측해서 틀리는 경우 loss가 더 크다\n",
        "#                                   이진분류 :  2-1) binary_crossentropy / 다중분류 : 2-2) categorical_crossentropy\n",
        "#-------------------------------------------------------------------------------------------------------------------------\n",
        "# * 최적화함수 - 딥러닝에서 학습속도를 빠르고 안정적이게 만드는것\n",
        "#                손실함수를 줄여나가면서 학습하는 방법은 어떤 optimizer를 사용하느냐에 따라 달라진다\n",
        "# 1) 확률적 경사 하강법(Stochastic Gradient Descent, SGD)\n",
        "#    - 매개변수 값을 조정시 전체 데이터가 아니라 랜덤으로 선택한 하나의 데이터에 대해서만 계산하는 방법 더 적은\n",
        "#      데이터를 사용하므로 더 빠르게 계산할 수 있다\n",
        "# 2) Adagrad - 적응형 경사(Adaptive Gradient)의 약자로, 경사하강법(Gradient Descent)의 일종이며,\n",
        "#              변수의 변화 정도에 따라 학습률(learning rate)을 조절해 나가는 최적화 방식이다\n",
        "# 3) RMSprop - 학습이 진행될수록 가중치 업데이트 강도가 약해지는 Adagrad 의 단점을 보완하고자 제안된 방법 /\n",
        "#              과거의 gradient 값은 잊고 새로운 gradient 값을 크게 반영하여 가중치를 업데이트\n",
        "# 4) 모멘텀(Momentum) - 가중치를 업데이트할때 이전 가중치 업데이트값의 일정 비율을 더하는것\n",
        "#                       가중치가 local minimum 에 빠지지 않고 학습하는 방향대로 가도록하는 방법\n",
        "# 5) 아담(Adam) - RMSprop와 모멘텀을 합친 방법 / 방향과 학습률 두 가지를 모두 잡기 위한 방법\n",
        "#==========================================================================================================================\n",
        "\n",
        "#model.compile(loss='mean_squared_error',\n",
        "#              optimizer='sgd',\n",
        "#              metrics=['accuracy'])\n",
        "\n",
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=30, batch_size=2)\n",
        "\n",
        "#=========================================================================\n",
        "pred = model.predict(x_test)\n",
        "print('x_test : ', x_test.flatten(), '예측 : ', pred.flatten())\n"
      ]
    }
  ]
}