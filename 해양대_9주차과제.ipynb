{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFgabawkcNrexlpe7cnR2N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimyt1976/AI-Programming/blob/main/%ED%95%B4%EC%96%91%EB%8C%80_9%EC%A3%BC%EC%B0%A8%EA%B3%BC%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9s9aUQzXSbr",
        "outputId": "115bb3f8-ebae-44ea-f139-68410554fee8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train :  [0 1 2] y_train :  [1 4 7]\n",
            "x_test :  [3 4 5] y_test :  [10 13 16]\n",
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_48 (Dense)            (None, 4)                 8         \n",
            "                                                                 \n",
            " dense_49 (Dense)            (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13 (52.00 Byte)\n",
            "Trainable params: 13 (52.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 3.2387 - accuracy: 0.0000e+00\n",
            "Epoch 2/30\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 3.0951 - accuracy: 0.0000e+00\n",
            "Epoch 3/30\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.9478 - accuracy: 0.0000e+00\n",
            "Epoch 4/30\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.8164 - accuracy: 0.0000e+00\n",
            "Epoch 5/30\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.6609 - accuracy: 0.0000e+00\n",
            "Epoch 6/30\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.5215 - accuracy: 0.0000e+00\n",
            "Epoch 7/30\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 2.3924 - accuracy: 0.0000e+00\n",
            "Epoch 8/30\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 2.2523 - accuracy: 0.0000e+00\n",
            "Epoch 9/30\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 2.1152 - accuracy: 0.0000e+00\n",
            "Epoch 10/30\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.9819 - accuracy: 0.3333\n",
            "Epoch 11/30\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.8284 - accuracy: 0.3333\n",
            "Epoch 12/30\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.6602 - accuracy: 0.3333\n",
            "Epoch 13/30\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.4849 - accuracy: 0.3333\n",
            "Epoch 14/30\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.2753 - accuracy: 0.3333\n",
            "Epoch 15/30\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.0554 - accuracy: 0.3333\n",
            "Epoch 16/30\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.8244 - accuracy: 0.3333\n",
            "Epoch 17/30\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.5996 - accuracy: 0.3333\n",
            "Epoch 18/30\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4949 - accuracy: 0.3333\n",
            "Epoch 19/30\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3613 - accuracy: 0.3333\n",
            "Epoch 20/30\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2161 - accuracy: 0.3333\n",
            "Epoch 21/30\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0731 - accuracy: 0.3333\n",
            "Epoch 22/30\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2080 - accuracy: 0.3333\n",
            "Epoch 23/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0992 - accuracy: 0.3333\n",
            "Epoch 24/30\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0788 - accuracy: 0.3333\n",
            "Epoch 25/30\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.1803 - accuracy: 0.3333\n",
            "Epoch 26/30\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.0363 - accuracy: 0.3333\n",
            "Epoch 27/30\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.1650 - accuracy: 0.3333\n",
            "Epoch 28/30\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.0519 - accuracy: 0.3333\n",
            "Epoch 29/30\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0301 - accuracy: 0.3333\n",
            "Epoch 30/30\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.0697 - accuracy: 0.3333\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "x_test :  [3 4 5] 예측 :  [ 9.682222 12.602649 15.523076]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "x_train = np.array([[0], [1], [2]])\n",
        "y_train = 3 * x_train + 1\n",
        "\n",
        "x_test = np.array([[3], [4], [5]])\n",
        "y_test = 3 * x_test + 1\n",
        "\n",
        "print('x_train : ', x_train.flatten(), 'y_train : ', y_train.flatten())\n",
        "print('x_test : ', x_test.flatten(), 'y_test : ', y_test.flatten())\n",
        "\n",
        "#=========================================================================\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(4, input_shape=(1, )))\n",
        "model.add(keras.layers.Dense(1))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#==================================================================================================================================================================================================================\n",
        "# loss = mean_squared_error, binary_crossentropy, categorical_crossentropy, mean_absolute_error, huber_loss\n",
        "# optimizer = sgd, adam , adagrad , rmsprop\n",
        "#==================================================================================================================================================================================================================\n",
        "# * 손실함수 - 예측값과 실제값의 차이를 구하는 함수 / 두값의 차이가 클수록 손실함수의 값은 커짐 / 두값의 차이가 작을수록 손실함수의 값도 작아짐\n",
        "#              손실 함수의 값을 최소화 하는 W, b를 찾아가는것이 학습 목표 / 회귀 : 평균제곱오차 / 분류 : 크로스 엔트로피\n",
        "# 평균 제곱 오차 (Mean Squared Error, MSE) - 연속형 변수를 예측할 때 사용한다\n",
        "# 크로스 엔트로피 (Cross-Entropy) - 낮은 확률로 예측해서 맞추거나, 높은  확률로 예측해서 틀리는 경우 loss가 더 크다\n",
        "#                                   이진분류 :  binary_crossentropy / 다중분류 : categorical_crossentropy\n",
        "#                                   y : 실제값(0혹은1) / y^ : 예측값(확률)\n",
        "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# * 최적화함수 - 딥러닝에서 학습속도를 빠르고 안정적이게 만드는것\n",
        "#                손실함수를 줄여나가면서 학습하는 방법은 어떤 optimizer를 사용하느냐에 따라 달라진다\n",
        "# 확률적 경사 하강법(Stochastic Gradient Descent, SGD) - 매개변수 값을 조정시 전체 데이터가 아니라 랜덤으로 선택한 하나의 데이터에 대해서만 계산하는 방법 더 적은 데이터를 사용하므로 더 빠르게 계산할 수 있다\n",
        "# Adagrad - 적응형 경사(Adaptive Gradient)의 약자로, 경사하강법(Gradient Descent)의 일종이며, 변수의 변화 정도에 따라 학습률(learning rate)을 조절해 나가는 최적화 방식이다\n",
        "# RMSprop - 학습이 진행될수록 가중치 업데이트 강도가 약해지는 Adagrad 의 단점을 보완하고자 제안된 방법 / 과거의 gradient 값은 잊고 새로운 gradient 값을 크게 반영하여 가중치를 업데이트\n",
        "# 아담(Adam) - RMSprop와 모멘텀을 합친 방법 / 방향과 학습률 두 가지를 모두 잡기 위한 방법\n",
        "#==================================================================================================================================================================================================================\n",
        "\n",
        "#model.compile(loss='mean_squared_error',\n",
        "#              optimizer='sgd',\n",
        "#              metrics=['accuracy'])\n",
        "\n",
        "model.compile(loss='mean_absolute_error',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=30, batch_size=2)\n",
        "\n",
        "#=========================================================================\n",
        "pred = model.predict(x_test)\n",
        "print('x_test : ', x_test.flatten(), '예측 : ', pred.flatten())"
      ]
    }
  ]
}